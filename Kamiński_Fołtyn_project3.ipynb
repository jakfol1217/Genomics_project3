{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d028b3c-b2bc-4d8d-a3ff-5cf1d6b96350",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234cb364-05f4-4cf7-b7a7-9562c2f0c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc725c4d-0c5b-4bcf-815e-d944fea66b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_reads = pysam.AlignmentFile(\"SRR_final_sorted.bam\", \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f74248a-cefd-406b-a5f8-10a0f1db3994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4656238"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_reads.count() # how many reads are in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37a2ba3c-fb46-44c4-bfd9-a49c9d016fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bamfile_contigs = []\n",
    "for contig in sequence_reads.header.to_dict()['SQ']:\n",
    "    bamfile_contigs.append(contig['SN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f85be2-38b0-472e-8dae-2b9f029ecdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3366"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bamfile_contigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8980363-260d-4949-adde-f3e1c7915e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_read = next(sequence_reads.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32137c57-1d2f-4950-a2b8-d227ca073bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRR590764.636676\t99\t#0\t10354\t9\t43M1D33M\t#0\t10374\t89\tCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCC\tarray('B', [31, 30, 28, 31, 29, 32, 34, 32, 32, 31, 30, 32, 34, 31, 31, 34, 18, 28, 32, 31, 34, 33, 28, 31, 30, 32, 32, 34, 19, 27, 27, 20, 32, 32, 31, 27, 32, 20, 27, 31, 32, 30, 30, 33, 9, 18, 33, 11, 32, 27, 23, 22, 31, 26, 14, 22, 22, 24, 26, 28, 31, 31, 22, 28, 16, 29, 32, 33, 28, 32, 2, 2, 2, 2, 2, 2])\t[('MC', '69M7S'), ('BD', 'MMLNONNOLNNMMNLNNLMMLNMLLMKMMLLMLNNMMNLLNNMMNLNNMMNLNNMMNLNNMMNLNONNOMNNMMMM'), ('MD', '43^C33'), ('RG', 'SRR'), ('BI', 'PPNQPNPOMQPNOOMPONOOMPOOOOMPONOPMQPOOPNNQPOPPNQPOPPNQPOPPNQPOPPNQPPQPNQQPPPP'), ('NM', 1), ('MQ', 9), ('AS', 69), ('XS', 76)]\n"
     ]
    }
   ],
   "source": [
    "print(example_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960e94f-79d9-4f23-bc68-5f49b6a08b04",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb31ac1a-5e4c-4a0d-af87-9e2f3fe6f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeed49d5-2372-469a-b98f-c6eea4d5293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_VCF(filename, reference, variants):\n",
    "    write_header(filename, reference, list(variants.keys()), [v[\"contig_len\"] for v in variants.values()])\n",
    "    i = 1\n",
    "    with open(f\"{filename}.vcf\", \"a\") as f:\n",
    "        for key in variants.keys():\n",
    "            for n, q, b, r, v in zip(variants[key][\"num_reads\"], \\\n",
    "                                     variants[key][\"read_qualities\"], \\\n",
    "                                     variants[key][\"bin_bounds\"], \\\n",
    "                                     variants[key][\"refs\"], \\\n",
    "                                     variants[key][\"variants\"]\n",
    "                                    ):\n",
    "                if v == \"deletion\":\n",
    "                    f.write(f\"{key}\\t{b[0]+1}\\tDEL{str(i).zfill(10)}\\t{r.upper()}\\t<DEL>\\t100\\tPASS\\tSVTYPE=DEL;SVMETHOD=depthRead;END={b[1]};MAPQ={q}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{key}\\t{b[0]+1}\\tDUP{str(i).zfill(10)}\\t{r.upper()}\\t<DUP>\\t100\\tPASS\\tSVTYPE=DUP;SVMETHOD=depthRead;END={b[1]};MAPQ={q};SVLEN={b[1]-b[0]}\\n\")\n",
    "                i += 1\n",
    "            \n",
    "from datetime import datetime\n",
    "\n",
    "def write_header(filename, reference, contigs, contig_lengths):\n",
    "    with open(f\"{filename}.VCF\", \"w\") as f:\n",
    "        f.write('##fileformat=VCFv4.2\\n')\n",
    "        f.write(f'##fileDate={datetime.today().strftime(\"%Y%m%d\")}\\n')\n",
    "        f.write(f'##reference={reference}\\n')\n",
    "        f.write('##ALT=<ID=DUP,Description=\"Duplication\">\\n')\n",
    "        f.write('##ALT=<ID=DEL,Description=\"Deletion\">\\n')\n",
    "        f.write('##FILTER=<ID=PASS,Description=\"All filters passed\">\\n')\n",
    "        f.write('##INFO=<ID=SVTYPE,Number=1,Type=String,Description=\"Type of structural variant\">\\n')\n",
    "        f.write('##INFO=<ID=SVMETHOD,Number=1,Type=String,Description=\"Type of approach used to detect SV\">\\n')\n",
    "        f.write('##INFO=<ID=END,Number=1,Type=Integer,Description=\"1-based end position of the structural variant\">\\n')\n",
    "        f.write('##INFO=<ID=MAPQ,Number=1,Type=Integer,Description=\"Median mapping quality of reads\">\\n')\n",
    "        f.write('##INFO=<ID=SVLEN,Number=1,Type=Integer,Description=\"Duplication length for SVTYPE=DUP.\">\\n')\n",
    "        for contig, ln in zip(contigs, contig_lengths):\n",
    "            f.write(f'##contig=<ID={contig},length={ln}>\\n')\n",
    "        f.write('#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deb6178c-b069-4a9e-9527-5cb90aa556fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBS:\n",
    "    # Based on instructions and implementation from this blog: https://jeremy9959.net/Blog/cbs-fixed/\n",
    "    def __init__(self, shuffles=100, p=0.05, validation_p=0.01, min_size=5):\n",
    "        self.shuffles = shuffles\n",
    "        self.p = p\n",
    "        self.validation_p=validation_p\n",
    "        self.min_size = min_size\n",
    "        \n",
    "    @staticmethod\n",
    "    def t_statistic(x0, x1):\n",
    "        denom = np.sqrt((np.std(x0)**2)/len(x0) + (np.std(x1)**2)/len(x1)) + 1e-10\n",
    "        if len(x0) == 0:\n",
    "            mu0 = 0\n",
    "        else:\n",
    "            mu0 = np.mean(x0)\n",
    "        if len(x1) == 0:\n",
    "            mu1 = 0\n",
    "        else:\n",
    "            mu1 = np.mean(x1)\n",
    "        return ((mu0-mu1)/denom)**2\n",
    "\n",
    "    def _best_interval(self, x):\n",
    "        # shortcut for selecting best \"inside\" interval, taken from https://jeremy9959.net/Blog/cbs-fixed/\n",
    "        n = len(x)\n",
    "        x0 = x - np.mean(x)\n",
    "        y = np.cumsum(x0)\n",
    "        e0, e1 = np.argmin(y), np.argmax(y)\n",
    "        i0, i1 = min(e0, e1), max(e0, e1)\n",
    "        t = (y[i1]-y[i0])**2*n/(i1-i0+1)/(n+1-i1+i0)\n",
    "        return t, i0, i1+1\n",
    "\n",
    "    def _main_alg(self, x):\n",
    "        initial_t, initial_start, initial_end = self._best_interval(x)\n",
    "        if initial_end-initial_start == len(x):\n",
    "            return False, initial_t, initial_start, initial_end\n",
    "        if initial_start < self.min_size:\n",
    "            initial_start = 0\n",
    "        if len(x)-initial_end < self.min_size:\n",
    "            initial_end = len(x)\n",
    "        interval_significance = self._check_if_interval_significant(x, initial_t)\n",
    "        return interval_significance, initial_t, initial_start, initial_end\n",
    "\n",
    "    def _check_if_interval_significant(self, x, initial_threshold, seg=None):\n",
    "        x_copy = x.copy()\n",
    "        t_c = 0\n",
    "        if seg is not None:\n",
    "            thresh_tolerance = self.shuffles*self.validation_p\n",
    "        else:\n",
    "            thresh_tolerance = self.shuffles*self.p\n",
    "            \n",
    "        for _ in range(self.shuffles):\n",
    "            np.random.shuffle(x_copy)\n",
    "            if seg is None:\n",
    "                t, a, b = self._best_interval(x_copy)\n",
    "            else:\n",
    "                t = CBS.t_statistic(x_copy[:seg], x_copy[seg:])\n",
    "            if t >= initial_threshold:\n",
    "                t_c += 1\n",
    "            if t_c > thresh_tolerance:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def _recurrent_segment(self, x, start, end, segments):\n",
    "        threshold, t, s, e = self._main_alg(x[start:end])\n",
    "        if  not threshold or e-s < self.min_size or e-s == end-start :\n",
    "            segments.append((start, end))\n",
    "        else:\n",
    "            if s > 0:\n",
    "                self._recurrent_segment(x, start, start+s, segments)\n",
    "            if e-s > 0:\n",
    "                self._recurrent_segment(x, start+s, start+e, segments)\n",
    "            if start+e < end:\n",
    "                self._recurrent_segment(x, start+e, end, segments)\n",
    "        return segments\n",
    "\n",
    "    def validate(self, x, L):\n",
    "        S = [x[0] for x in L]+[len(x)]\n",
    "        SV = [0]\n",
    "        left = 0\n",
    "        for test, s in enumerate(S[1:-1]):\n",
    "            x0 = x[S[left]:S[test+2]]\n",
    "            i = S[test+1]-S[left]\n",
    "            t = CBS.t_statistic(x0[:i], x0[i:])\n",
    "            if self._check_if_interval_significant(x0, t, i):\n",
    "                SV.append(S[test+1])\n",
    "                left += 1\n",
    "        SV.append(S[-1])\n",
    "        return SV\n",
    "\n",
    "    def run_algorithm(self, counts, contig):\n",
    "        print(f\"Segmentation started for contig: {contig}\\n\")\n",
    "        seg = {contig:[]}\n",
    "        if len(counts) <= 1:\n",
    "            return seg\n",
    "        self._recurrent_segment(counts, 0, len(counts), seg[contig])\n",
    "        print(f\"Starting breakpoint validation for contig {contig}...\\n\")\n",
    "        seg[contig] = self.validate(counts, seg[contig])\n",
    "        return seg\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e598dd-514c-4681-abb9-c606988e5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import statistics\n",
    "import multiprocessing as mp\n",
    "from Bio import SeqIO\n",
    "\n",
    "class ReadDepthAlgorithm:\n",
    "    def __init__(self, bin_size, file, reference, output_filename=None, average=True, dup_threshold=None, del_threshold=None, \n",
    "                 check_validity = False, cbs=None, num_cpus=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        bin_size := int, required, the size of the bin\n",
    "        file := string, required, .bam file name with reads for variant detection\n",
    "        reference := string, required, .fasta file name with reference genome sequences\n",
    "        output_filename := string or None, the filename of the output .vcf file\n",
    "        average := boolean, whether to average the number of reads across segments\n",
    "        dup_threshold := float or None, the thershold for duplication\n",
    "        del_threshold := float or None, the thershold for deletions\n",
    "        check_validity := bool, whether to check the validity of the reads (if True, unmapped and duplicated reads will not be considered for the algorithm)\n",
    "        cbs := CBS or None, cbs algorithm class\n",
    "        num_cpus := int or None, the number of cpus to use during parallelization of CBS algorithm, if None uses all available\n",
    "        \"\"\"\n",
    "        self.bin_size = bin_size\n",
    "        self.dup_threshold = dup_threshold # by default the thresholds will be the 95 and 5 percentiles of the number of reads per bin\n",
    "        self.del_threshold = del_threshold\n",
    "        self.bins = dict()\n",
    "        self.check_validity = check_validity\n",
    "        self.average = average\n",
    "        self.file = file\n",
    "        self.reference = reference\n",
    "        self.reference_dict = self._create_reference_dict()\n",
    "        self.output_filename = output_filename\n",
    "        if num_cpus is None:\n",
    "            self.num_cpus = mp.cpu_count()\n",
    "        else:\n",
    "            self.num_cpus = num_cpus\n",
    "        if cbs is None:\n",
    "            self.cbs = CBS(**kwargs)\n",
    "        else:\n",
    "            self.cbs = cbs\n",
    "\n",
    "    def _create_reference_dict(self):\n",
    "        # creating a dictionary containing reference sequences for each contig\n",
    "        fasta_file = SeqIO.parse(open(self.reference), 'fasta')\n",
    "        reference_dict = dict()\n",
    "        for fasta in fasta_file:\n",
    "            reference_dict[fasta.id] = fasta.seq\n",
    "        return reference_dict\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_read_start_len(read):\n",
    "        return read.reference_start, read.reference_length\n",
    "\n",
    "    @staticmethod\n",
    "    def read_in_bin(bin_start, bin_end, read): #check if read falls inside a given bin. \n",
    "        # We assume that read falls inside a bin if at least half of it belongs to that bin\n",
    "        read_start, read_len = ReadDepthAlgorithm.get_read_start_len(read)\n",
    "        if read_len is None:\n",
    "            return False\n",
    "        read_end = read_start + read_len\n",
    "        return bin_start - read_start <= read_len/2 and read_end - bin_end <= read_len/2  \n",
    "\n",
    "    def valid_read(self, read):\n",
    "        # if we check validity, we take into account only mapped and non-duplicate reads <- as far as I understand, \n",
    "        #we take all the reads, no matter the mappability (authors of ReadDepth multiplied by the inverse of mappability to account for unmapped reads)\n",
    "        if not self.check_validity:\n",
    "            return True # if we dont check validity, all reads are passed as valid\n",
    "        return not read.is_unmapped and not read.is_duplicate # and read.is_read1 <- we will read both reads and their mates\n",
    "    \n",
    "\n",
    "    def _set_thresholds(self):\n",
    "        # setting duplication and deletion thresholds if None\n",
    "        all_bin_nums = []\n",
    "        for contig in self.bins:\n",
    "            all_bin_nums.extend(self.bins[contig]['num_reads'])\n",
    "        if self.dup_threshold is None:\n",
    "            self.dup_threshold = np.quantile(all_bin_nums, 0.95)\n",
    "        if self.del_threshold is None:\n",
    "            self.del_threshold = np.quantile(all_bin_nums, 0.05)\n",
    "    \n",
    "    def _handle_segmented_vals(self, result):\n",
    "        # function for assigning the results of the CBS algorithm to appropriate contigs\n",
    "        contig = next(iter(result))\n",
    "        self.bins[contig][\"segments\"] = result[contig]\n",
    "    \n",
    "\n",
    "    def _run_segmentation(self, contig_list):\n",
    "        # function for running the CBS algorithm. It uses multiprocessing for algorithm parallelization\n",
    "        print(\"SEGMENTATION PROCESS...\")\n",
    "        args = []\n",
    "        for contig in contig_list:\n",
    "            reads = self.bins[contig][\"num_reads\"]\n",
    "            args.append((reads, contig))\n",
    "        with mp.Pool(self.num_cpus) as p: \n",
    "            for result in p.starmap(self.cbs.run_algorithm, args):\n",
    "                self._handle_segmented_vals(result)\n",
    "                \n",
    "\n",
    "    def _divide_reads_into_bins(self, contig_list):\n",
    "        # compute number of reads in bins for all contigs\n",
    "        print(\"BINNING PROCESS...\")\n",
    "        if self.reference_dict is None:\n",
    "            self.reference_dict = self._create_reference_dict()\n",
    "        for contig in tqdm(contig_list):\n",
    "            self._divide_reads_into_bins_one_contig(contig)\n",
    "        self.reference_dict = None\n",
    "    \n",
    "    def _divide_reads_into_bins_one_contig(self, contig):\n",
    "        # compute number of reads in bins for a given contig\n",
    "        contig_len = self.file.get_reference_length(contig)\n",
    "        num_bins = contig_len//self.bin_size\n",
    "        self.bins[contig] = dict()\n",
    "        self.bins[contig][\"contig_len\"] = contig_len\n",
    "        self.bins[contig][\"num_reads\"] = []\n",
    "        self.bins[contig][\"read_qualities\"] = []\n",
    "        self.bins[contig][\"bin_bounds\"] = []\n",
    "        self.bins[contig][\"refs\"] = []\n",
    "        seq = None\n",
    "        if contig in self.reference_dict:\n",
    "            seq = self.reference_dict[contig]\n",
    "\n",
    "        for i in tqdm(range(1, num_bins+1), leave=False):\n",
    "            bin_start = (i - 1) * self.bin_size\n",
    "            bin_end = i * self.bin_size\n",
    "            num_reads = 0\n",
    "            read_qualities = []\n",
    "            for read in self.file.fetch(contig, bin_start, bin_end):\n",
    "                if ReadDepthAlgorithm.read_in_bin(bin_start, bin_end, read) and self.valid_read(read):\n",
    "                    num_reads += 1\n",
    "                    read_qualities.append(read.mapping_quality)\n",
    "            self.bins[contig][\"num_reads\"].append(num_reads)\n",
    "            self.bins[contig][\"read_qualities\"].append(read_qualities)\n",
    "            self.bins[contig][\"bin_bounds\"].append((bin_start, bin_end))\n",
    "            if seq is not None:\n",
    "                self.bins[contig][\"refs\"].append(seq[bin_start])\n",
    "        del seq\n",
    "            \n",
    "    def get_bins(self):\n",
    "        # equivalent to get_SVs()\n",
    "        return self.bins\n",
    "\n",
    "    def _filter_binned_variances(self, contig_list):\n",
    "        # applies the filtering to all the contigs\n",
    "        print(\"FILTERING BINS...\")\n",
    "        for contig in contig_list:\n",
    "            self._filter_binned_variances_one_contig(contig)\n",
    "\n",
    "    def _filter_binned_variances_one_contig(self, contig):\n",
    "        # function for filtering out the non-variant segments in a particular contig\n",
    "        variants = []\n",
    "        idxes = []\n",
    "        for i, read in enumerate(self.bins[contig][\"num_reads\"]):\n",
    "            if read > self.dup_threshold:\n",
    "                idxes.append(i)\n",
    "                variants.append(\"duplication\")\n",
    "            if read <= self.del_threshold:\n",
    "                idxes.append(i)\n",
    "                variants.append(\"deletion\")\n",
    "        self.bins[contig][\"variants\"] = variants\n",
    "        if len(idxes) > 0:\n",
    "            self.bins[contig][\"num_reads\"]      = np.array(self.bins[contig][\"num_reads\"])[idxes]\n",
    "            self.bins[contig][\"read_qualities\"] = np.array(self.bins[contig][\"read_qualities\"])[idxes]\n",
    "            self.bins[contig][\"bin_bounds\"]     = np.array(self.bins[contig][\"bin_bounds\"])[idxes]\n",
    "        if len(self.bins[contig][\"refs\"]) > 0:\n",
    "            self.bins[contig][\"refs\"]           = np.array(self.bins[contig][\"refs\"])[idxes]\n",
    "        del self.bins[contig][\"segments\"]\n",
    "\n",
    "    def get_SVs(self):\n",
    "        # returns the found SVs\n",
    "        return self.bins\n",
    "\n",
    "    def _consolidate_variants(self, contig_bins):\n",
    "        # function for consolidating bins after CBS segmentation\n",
    "        bins      = contig_bins[\"num_reads\"]\n",
    "        segments  = contig_bins[\"segments\"]\n",
    "        qualities = contig_bins[\"read_qualities\"]\n",
    "        if len(bins) == 0 or len(segments) == 0:\n",
    "            if len(contig_bins[\"read_qualities\"]) != 0:\n",
    "                new_qualities = []\n",
    "                for q in contig_bins[\"read_qualities\"]:\n",
    "                    if len(q) == 0:\n",
    "                        new_qualities.append(0.0)\n",
    "                    else:\n",
    "                        new_qualities.append(statistics.median(q))\n",
    "                contig_bins[\"read_qualities\"] = new_qualities\n",
    "            else:\n",
    "                contig_bins[\"read_qualities\"] = [0.0]\n",
    "            return \n",
    "        new_variants = []\n",
    "        new_bounds = []\n",
    "        new_qualities = []\n",
    "        prev_segment = segments[0]\n",
    "        for segment in segments[1:]:\n",
    "            if self.average:\n",
    "                new_variants.append(round(np.sum(bins[prev_segment:segment])/(segment-prev_segment),1)) # we set segment read number as average number of reads from bins\n",
    "            else:\n",
    "                new_variants.append((np.sum(bins[prev_segment:segment])))\n",
    "            new_bounds.append((prev_segment*self.bin_size, segment*self.bin_size))\n",
    "            new_qualities.append(np.concatenate(qualities[prev_segment:segment]))\n",
    "            if len(new_qualities[-1]) == 0:\n",
    "                new_qualities[-1] = 0.0\n",
    "            else:\n",
    "                new_qualities[-1] = float(statistics.median(new_qualities[-1]))\n",
    "            prev_segment = segment\n",
    "        contig_bins[\"num_reads\"]      = new_variants\n",
    "        contig_bins[\"bin_bounds\"]     = new_bounds\n",
    "        contig_bins[\"read_qualities\"] = new_qualities\n",
    "        \n",
    "\n",
    "        # MAIN FUNCTION FOR RUNNING ALGORITHM\n",
    "    def run_algorithm(self, contig_list):\n",
    "        self._divide_reads_into_bins(contig_list)\n",
    "        self._run_segmentation(contig_list)\n",
    "        print(\"CONSOLIDATING BINS...\")\n",
    "        for contig in contig_list:\n",
    "            self._consolidate_variants(self.bins[contig])\n",
    "        self._set_thresholds()\n",
    "        self._filter_binned_variances(contig_list)\n",
    "        if self.output_filename:\n",
    "            write_VCF(self.output_filename, self.reference, self.bins)\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "496eaa3a-6b9c-4c72-a92a-9fc50fa2b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_FILE='GRCh38_full_analysis_set_plus_decoy_hla.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0da53f83-dd8b-450a-b151-e014dab6fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "rda = ReadDepthAlgorithm(1000, file=sequence_reads, reference=REFERENCE_FILE, average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49509cdf-076d-4647-b2a0-995ccda29add",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINNING PROCESS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117e01591f094716be91e4f8b44044aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEGMENTATION PROCESS...\n",
      "Segmentation started for contig: chr1\n",
      "\n",
      "Starting breakpoint validation for contig chr1...\n",
      "\n",
      "CONSOLIDATING BINS...\n",
      "FILTERING BINS...\n"
     ]
    }
   ],
   "source": [
    "rda.run_algorithm(bamfile_contigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab99717-8d0e-45dd-90d3-0811f21a648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_variants(bins): # for counting the number of detected variants\n",
    "    count_dict = {\n",
    "        \"deletion\":0,\n",
    "        \"duplication\":0\n",
    "    }\n",
    "    for contig in bins:\n",
    "        for var in bins[contig][\"variants\"]:\n",
    "            count_dict[var] += 1\n",
    "    print(f'Number of deletions={count_dict[\"deletion\"]}, number of duplications={count_dict[\"duplication\"]}')\n",
    "\n",
    "def count_variants_smaller(bins): # for counting the number of detected variants on only first 24 chromosomes\n",
    "    i = 0\n",
    "    count_dict = {\n",
    "        \"deletion\":0,\n",
    "        \"duplication\":0\n",
    "    }\n",
    "    for contig in bins:\n",
    "        for var in bins[contig][\"variants\"]:\n",
    "            count_dict[var] += 1\n",
    "        i += 1\n",
    "        if i == 23:\n",
    "            break\n",
    "    print(f'Number of deletions={count_dict[\"deletion\"]}, number of duplications={count_dict[\"duplication\"]}')\n",
    "\n",
    "def average_segment_length(bins): # for computing average segment length\n",
    "    lengths = []\n",
    "    for contig in bins:\n",
    "        for bb in bins[contig][\"bin_bounds\"]:\n",
    "            lengths.append(bb[1]-bb[0])\n",
    "    print(f\"Average segment length={np.mean(lengths)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
